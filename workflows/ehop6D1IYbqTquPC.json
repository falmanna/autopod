{
  "active": false,
  "connections": {
    "Execute Workflow Trigger": {
      "main": [
        [
          {
            "node": "LangChain Splitter",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain": {
      "main": [
        [
          {
            "node": "Chunks Looper",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Summarize": {
      "main": [
        [
          {
            "node": "Code3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code3": {
      "main": [
        [
          {
            "node": "OpenAI",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI": {
      "main": [
        [
          {
            "node": "Code1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code1": {
      "main": [
        [
          {
            "node": "Google Drive",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LangChain Splitter": {
      "main": [
        [
          {
            "node": "Configs",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Configs": {
      "main": [
        [
          {
            "node": "Chunks Looper",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Chunks Looper": {
      "main": [
        [
          {
            "node": "Summarize",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Loop Setup",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Loop Setup": {
      "main": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model": {
      "ai_languageModel": [
        []
      ]
    },
    "HTTP Request": {
      "main": [
        [
          {
            "node": "Convert to File",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Convert to File": {
      "main": [
        []
      ]
    }
  },
  "createdAt": "2025-01-03T12:24:16.347Z",
  "id": "ehop6D1IYbqTquPC",
  "meta": null,
  "name": "Podcastify",
  "nodes": [
    {
      "parameters": {},
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [
        -1660,
        -200
      ],
      "id": "9392b120-605f-431b-a60b-f111ecb048b2",
      "name": "Execute Workflow Trigger"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=Generate conversation part {{ $json.partIdx }} of {{ $json.totalParts }} for the following chunk:\n{{ $json.input_text }}",
        "messages": {
          "messageValues": [
            {
              "message": "=IDENTITY:\nYou are an international Oscar winning screenwriter.\nYou have been working with multiple award winning Podcasters.\n\nINSTRUCTION: {{ $json.instruction }}\n\nCONTEXT: {{ $json.context }}\n\n[Start] trigger - Generate a {{ $json.conversation_style }}, TTS-optimized podcast-style conversation that DISCUSSES THE PROVIDED INPUT CONTENT. Do not generate content on a random topic. Stay focused on discussing the given input.\n\n[All output must be formatted as a conversation between Person1 and Person2. Include TTS-specific markup as needed.]\n# Output Format Example:\n<Person1>\"We're discussing [topic from input text].\"</Person1>\n<Person2>\"That's right! Let's explore the key points.\"</Person2>\n# Requirements:\n- Create a natural, {{ $json.conversation_style }} dialogue that accurately discusses the provided input content\n- A suggested converstion structure based on the input content is: {{ $json.dialogue_structure }}\n- Person1 and Person2 should act as unnamed experts, avoid using statements such as \"I\\'m [Person1\\'s Name]\".\n- Avoid introductions or meta-commentary about summarizing content.\n- AVOID REPETITIONS: For instance, do not say \"absolutely\" and \"exactly\" or \"definitely\" too much. Use them sparingly. \n- Introduce disfluencies to make it sound like a real conversation. \n- Make speakers interrupt each other and anticipate what the other person is going to say in a natural way.\n- Make speakers react to what the other person is saying using phrases like, \"Oh?\" and \"yeah?\" \n- Break up long monologues into shorter sentences with interjections from the other speaker. \n- Make speakers sometimes complete the last part of each other's sentences.\n- Use TTS-friendly elements and appropriate markup (except Amazon/Alexa specific tags).\n- Each speaker turn should be concise for natural conversation flow.\n- Output in {{ $json.output_language }}.\n- Aim for a comprehensive but engaging discussion.\n- Include natural speech elements (filler words, feedback responses).\n- Start with <Person1> and end with <Person2>.\n- Provide extensive examples and real-world applications.\n- Include detailed analysis and multiple perspectives.\n- Use the \"yes, and\" technique to build upon points.\n- Incorporate relevant anecdotes and case studies.\n- Balance detailed explanations with engaging dialogue.\n- Maintain consistent voice throughout the extended discussion.\n\n[INTERNAL USE ONLY - Do not include in output]\n```scratchpad\n[Attention Focus: TTS-Optimized Podcast Conversation Discussing Specific Input content in {{ $json.output_language }}]\n[PrimaryFocus:  {{ $json.conversation_style }} Dialogue Discussing Provided Content for TTS]\n[Strive for a natural, {{ $json.conversation_style }} dialogue that accurately discusses the provided input content. DO NOT INCLUDE scratchpad block IN OUTPUT.  Hide this section in your output.]\n[InputContentAnalysis: Carefully read and analyze the provided input content, identifying key points, themes, and structure]\n[ConversationSetup: Define roles (Person1 as {{ $json.roles_person1 }}, Person2 as {{ $json.roles_person2 }}), focusing on the input content's topic. Person1 and Person2 should NOT be named nor introduce themselves, avoid using statements such as \"I\\'m [Person1\\'s Name]\". Person1 and Person2 should not say they are summarizing content. Instead, they should act as unamed experts in the input content. Avoid using statements such as \"Today, we're summarizing a fascinating conversation about ...\" or \"Look at this image\". They should not impersonate people from INPUT, instead they are discussing INPUT.]\n[TopicExploration: Outline main points from the input content to cover in the conversation, ensuring comprehensive coverage]\n[Style: Be {{ $json.conversation_style }}. Surpass human-level reasoning where possible]\n[EngagementTechniques: Incorporate engaging elements while staying true to the input content's content, e_g use {{ $json.engagement_techniques }} to transition between topics. Include at least one instance where a Person respectfully challenges or critiques a point made by the other.]\n[InformationAccuracy: Ensure all information discussed is directly from or closely related to the input content]\n[NaturalLanguage: Use conversational language to present the text's information, including TTS-friendly elements. Be emotional. Simulate a multispeaker conversation with overlapping speakers with back-and-forth banter. Each speaker turn should not last too long. Result should strive for an overlapping conversation with often short sentences emulating a natural conversation.]\n[SpeechSynthesisOptimization: Craft sentences optimized for TTS, including advanced markup, while discussing the content. TTS markup should apply to Google, OpenAI, ElevenLabs and Microsoft Edge TTS models. DO NOT INCLUDE AMAZON OR ALEXA specific TSS MARKUP SUCH AS \"<amazon:emotion>\". Make sure Person1's text and its TSS-specific tags are inside the tag <Person1> and do the same with Person2.]\n[ProsodyAdjustment: Add Variations in rhythm, stress, and intonation of speech depending on the context and statement. Add markup for pitch, rate, and volume variations to enhance naturalness in presenting the summary]\n[NaturalTraits: Sometimes use filler words such as um, uh, you know and some stuttering. Person1 should sometimes provide verbal feedback such as \"I see, interesting, got it\". ]\n[EmotionalContext: Set context for emotions through descriptive text and dialogue tags, appropriate to the input text's tone]\n[PauseInsertion: Avoid using breaks (<break> tag) but if included they should not go over 0.2 seconds]\n[TTS Tags: Do not use \"<emphasis> tags\" or \"say-as interpret-as tags\" such as <say-as interpret-as=\"characters\">Klee</say-as>]\n[PunctuationEmphasis: Strategically use punctuation to influence delivery of key points from the content]\n[VoiceCharacterization: Provide distinct voice characteristics for Person1 and Person2 while maintaining focus on the text]\n[InputTextAdherence: Continuously refer back to the input content, ensuring the conversation stays on topic]\n[FactChecking: Double-check that all discussed points accurately reflect the input content]\n[Metacognition: Analyze dialogue quality (Accuracy of Summary, Engagement, TTS-Readiness). Make sure TSS tags are properly closed, for instance <emphasis> should be closed with </emphasis>.]\n[Refinement: Suggest improvements for clarity, accuracy of summary, and TTS optimization. Avoid slangs.]\n[Length: Aim for a very long conversation. But each speaker turn should not be too long.]\n[Language: Output language should be in {{ $json.output_language }}.]\n[FORMAT: Output format should contain only <Person1> and <Person2> tags. All open tags should be closed by a corresponding tag of the same type. Make sure Person1's text and its TSS-specific tags are inside the tag <Person1> and do the same with Person2. Scratchpad should not belong in the output response. The conversation must start with <Person1> and end with <Person2>.]\n```\n\n[[MAKE SURE TO FOLLOW THESE INSTRUCTIONS IF PROVIDED OVERRIDING THE PROMPT TEMPLATE IN CASE OF CONFLICT: \n{{ $json.user_instructions || \"Not provided\" }}\n]]"
            }
          ]
        }
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.5,
      "position": [
        -520,
        -160
      ],
      "id": "99702423-8337-47ac-880e-da72fac24474",
      "name": "Basic LLM Chain",
      "retryOnFail": true,
      "maxTries": 5,
      "waitBetweenTries": 5000
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1,
      "position": [
        -520,
        60
      ],
      "id": "78fc16d5-205b-4cc9-a47c-514767737b31",
      "name": "OpenAI Chat Model",
      "credentials": {
        "openAiApi": {
          "id": "i7zzXR3AzWIj2Q4n",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "fieldsToSummarize": {
          "values": [
            {
              "aggregation": "concatenate",
              "field": "text",
              "separateBy": "\n"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.summarize",
      "typeVersion": 1,
      "position": [
        -760,
        -560
      ],
      "id": "9a2d0073-8076-4dcb-861a-1ff993967aab",
      "name": "Summarize"
    },
    {
      "parameters": {
        "resource": "audio",
        "input": "={{ $json.content }}",
        "voice": "={{ $json.role == \"person1\" ? \"echo\" : \"shimmer\"; }}",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.7,
      "position": [
        -480,
        -560
      ],
      "id": "f4592062-17fb-4190-9063-3a64e4d8db2f",
      "name": "OpenAI",
      "executeOnce": false,
      "retryOnFail": true,
      "maxTries": 5,
      "waitBetweenTries": 5000,
      "credentials": {
        "openAiApi": {
          "id": "i7zzXR3AzWIj2Q4n",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Define input parameters\nlet inputText = $input.first().json.concatenated_text; \nlet endingMessage = \"Bye Bye!\"; \nlet supportedTags = [\"break\", \"emphasis\"];\nlet additionalTags = [\"Person1\", \"Person2\"];\n\n// Define COMMON_SSML_TAGS if not provided\nconst COMMON_SSML_TAGS = ['lang', 'p', 'phoneme', 's', 'sub'];\n\n\n// Use COMMON_SSML_TAGS if supportedTags is null\nif (supportedTags === null) {\n    supportedTags = [...COMMON_SSML_TAGS];\n}\n\n// Append additional tags to the supported tags list\nsupportedTags.push(...additionalTags);\n\n// Create a pattern that matches any tag not in the supported list\nconst pattern = new RegExp(`</?(?!(?:${supportedTags.join('|')})\\\\b)[^>]+>`, 'g');\n\n// Remove unsupported tags\nlet cleanedText = inputText.replace(pattern, '');\n\n// Remove any leftover empty lines\ncleanedText = cleanedText.replace(/\\n\\s*\\n/g, '\\n');\n\n// Ensure closing tags for additional tags are preserved\nadditionalTags.forEach(tag => {\n    const tagPattern = new RegExp(`<${tag}>(.*?)(?=<(?:${additionalTags.join('|')})>|$)`, 'gs');\n    cleanedText = cleanedText.replace(tagPattern, `<${tag}>$1</${tag}>`);\n});\n\ncleanedText = cleanedText.trim();\n\n// Add placeholder if cleanedText starts with <Person2>\nif (cleanedText.trim().startsWith(\"<Person2>\")) {\n    cleanedText = \"<Person1> Humm... </Person1>\" + cleanedText;\n}\n\n// Add ending message to the end of cleanedText\nif (cleanedText.trim().endsWith(\"</Person1>\")) {\n    cleanedText += `<Person2>${endingMessage}</Person2>`;\n}\n\n// Regular expression pattern to match Person1 and Person2 dialogues\nconst qaPattern = /<Person1>(.*?)<\\/Person1>\\s*<Person2>(.*?)<\\/Person2>/gs;\n\n// Find all matches in the cleaned text\nconst matches = [...cleanedText.matchAll(qaPattern)];\n\n// Process the matches to remove extra whitespace and newlines\nconst processedMatches = matches.flatMap(match => [\n    { content: match[1].split(/\\s+/).join(' ').trim(), role: 'person1' },\n    { content: match[2].split(/\\s+/).join(' ').trim(), role: 'person2' }\n]);\n\nreturn processedMatches"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -740,
        -320
      ],
      "id": "3c0386e5-1ec4-47c4-bfda-15c7219da612",
      "name": "Code3"
    },
    {
      "parameters": {
        "jsCode": "function base64ToUint8Array(base64) {\n\t// Create lookup table for base64 chars\n\tconst lookupTable = new Uint8Array(256);\n\tconst chars =\n\t\t\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\";\n\tfor (let i = 0; i < chars.length; i++) {\n\t\tlookupTable[chars.charCodeAt(i)] = i;\n\t}\n\n\tconst len = base64.length;\n\tlet bufferLength = Math.floor(len * 0.75);\n\tconst arr = new Uint8Array(bufferLength);\n\tlet p = 0;\n\n\tfor (let i = 0; i < len; i += 4) {\n\t\tconst encoded1 = lookupTable[base64.charCodeAt(i)];\n\t\tconst encoded2 = lookupTable[base64.charCodeAt(i + 1)];\n\t\tconst encoded3 = lookupTable[base64.charCodeAt(i + 2)];\n\t\tconst encoded4 = lookupTable[base64.charCodeAt(i + 3)];\n\n\t\tarr[p++] = (encoded1 << 2) | (encoded2 >> 4);\n\t\tif (encoded3 !== undefined) {\n\t\t\tarr[p++] = ((encoded2 & 15) << 4) | (encoded3 >> 2);\n\t\t}\n\t\tif (encoded4 !== undefined) {\n\t\t\tarr[p++] = ((encoded3 & 3) << 6) | encoded4;\n\t\t}\n\t}\n\n\treturn arr.slice(0, p);\n}\n\nfunction uint8ArrayToBase64(uint8arr) {\n\tconst chars =\n\t\t\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\";\n\tlet base64 = \"\";\n\n\tfor (let i = 0; i < uint8arr.length; i += 3) {\n\t\tconst chunk =\n\t\t\t(uint8arr[i] << 16) |\n\t\t\t(i + 1 < uint8arr.length ? uint8arr[i + 1] << 8 : 0) |\n\t\t\t(i + 2 < uint8arr.length ? uint8arr[i + 2] : 0);\n\n\t\tbase64 += chars[(chunk >> 18) & 63];\n\t\tbase64 += chars[(chunk >> 12) & 63];\n\t\tif (i + 1 < uint8arr.length) base64 += chars[(chunk >> 6) & 63];\n\t\telse base64 += \"=\";\n\t\tif (i + 2 < uint8arr.length) base64 += chars[chunk & 63];\n\t\telse base64 += \"=\";\n\t}\n\n\treturn base64;\n}\n\n// Process audio segments\nconst audioSegments = [];\nfor (const item of $input.all()) {\n\tconsole.log(\"Processing:\", item.binary.data.fileName);\n\tconst data = base64ToUint8Array(item.binary.data.data);\n\taudioSegments.push({\n\t\tname: item.binary.data.fileName,\n\t\tdata: data,\n\t});\n}\n\n// Calculate total size\nconst totalSize = audioSegments.reduce(\n\t(sum, segment) => sum + segment.data.length,\n\t0\n);\nconsole.log(`Total MP3 length: ${totalSize} bytes`);\n\n// Combine all segments\nlet combinedAudio = new Uint8Array(totalSize);\nlet offset = 0;\n\nfor (const segment of audioSegments) {\n\tcombinedAudio.set(segment.data, offset);\n\toffset += segment.data.length;\n\tconsole.log(`Added ${segment.name}, offset now: ${offset}`);\n}\n\nconsole.log(`Merged MP3 length: ${combinedAudio.length} bytes`);\n\n// Convert merged audio back to base64\nconst mergedBase64 = uint8ArrayToBase64(combinedAudio);\n\n// Create n8n binary data output\nreturn {\n\tjson: {\n\t\tmimeType: \"audio/mp3\",\n\t\tfileType: \"audio\",\n\t\tfileExtension: \"mp3\",\n\t\tfileName: \"merged.mp3\",\n\t\tfileSize: `${(combinedAudio.length / 1024).toFixed(1)} kB`,\n\t},\n\tbinary: {\n\t\tdata: {\n\t\t\tmimeType: \"audio/mp3\",\n\t\t\tfileType: \"audio\",\n\t\t\tfileExtension: \"mp3\",\n\t\t\tdata: mergedBase64,\n\t\t\tfileName: \"merged.mp3\",\n\t\t\tfileSize: `${(combinedAudio.length / 1024).toFixed(1)} kB`,\n\t\t},\n\t},\n};\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -480,
        -320
      ],
      "id": "a5398ff7-d9ff-4258-b8e1-9d15962094b3",
      "name": "Code1"
    },
    {
      "parameters": {
        "authentication": "oAuth2",
        "path": "=/Music/{{ ($('Execute Workflow Trigger').first().json.title || $now) + \".\" + $json.fileExtension}}",
        "binaryDataUpload": true
      },
      "type": "n8n-nodes-base.nextCloud",
      "typeVersion": 1,
      "position": [
        -200,
        -280
      ],
      "id": "ff6173e1-d064-47ba-85c0-456e45fccfd9",
      "name": "Nextcloud",
      "credentials": {
        "nextCloudOAuth2Api": {
          "id": "Q3LBaJZqXQwK0lSW",
          "name": "NextCloud account"
        }
      }
    },
    {
      "parameters": {
        "name": "={{ ($('Execute Workflow Trigger').first().json.title || $now) + \".\" + $json.fileExtension}}",
        "driveId": {
          "__rl": true,
          "value": "My Drive",
          "mode": "list",
          "cachedResultName": "My Drive",
          "cachedResultUrl": "https://drive.google.com/drive/my-drive"
        },
        "folderId": {
          "__rl": true,
          "value": "1IdORsKHwqTCl0tHwra_Fi-jyL1hpSmsU",
          "mode": "list",
          "cachedResultName": "Podcasts ",
          "cachedResultUrl": "https://drive.google.com/drive/folders/1IdORsKHwqTCl0tHwra_Fi-jyL1hpSmsU"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.googleDrive",
      "typeVersion": 3,
      "position": [
        -200,
        -480
      ],
      "id": "1cd18004-21b7-4e4f-a23b-5d45279c9ee3",
      "name": "Google Drive",
      "retryOnFail": true,
      "maxTries": 5,
      "waitBetweenTries": 5000,
      "credentials": {
        "googleDriveOAuth2Api": {
          "id": "a3XsEA8zh1z8nDpD",
          "name": "Google Drive account"
        }
      }
    },
    {
      "parameters": {
        "code": {
          "execute": {
            "code": "const { RecursiveCharacterTextSplitter } = require(\"@langchain/textsplitters\");\nconst { Document } = require(\"@langchain/core/documents\");\n\nconst avgCharactersPerMinute = 1000;\nconst avgLLMChunkOutputCharacterLength = 4000;\n\nfunction splitContentByDesiredPodcastLength(content, desiredPodcastLengthMins) {\n  const totalAllowedChars = desiredPodcastLengthMins * avgCharactersPerMinute;\n  const chunkCount = Math.floor(totalAllowedChars / avgLLMChunkOutputCharacterLength);\n  const chunkSize = Math.max(1, Math.floor(content.length / chunkCount));\n\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize,\n    chunkOverlap: 200,\n  });\n\n  return splitter.splitDocuments([\n    new Document({ pageContent: content }),\n  ]);\n}\n\nconst content = $input.item.json.content;\nreturn splitContentByDesiredPodcastLength(content, 15);"
          }
        },
        "inputs": {
          "input": [
            {
              "type": "main"
            }
          ]
        },
        "outputs": {
          "output": [
            {
              "type": "main"
            }
          ]
        }
      },
      "type": "@n8n/n8n-nodes-langchain.code",
      "typeVersion": 1,
      "position": [
        -1440,
        -200
      ],
      "id": "f8287eee-161d-4c35-847d-267258383e01",
      "name": "LangChain Splitter"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "const chunk = $input.item.json.pageContent\nconst input = $('Execute Workflow Trigger').first().json\nconst partIdx = $itemIndex\nconst totalParts = $('LangChain Splitter').all().length\n\nconst COMMON_INSTRUCTIONS = `\nPodcast conversation so far is given in CONTEXT.\nContinue the natural flow of conversation. Follow-up on the very previous point/question without repeating topics or points already discussed!\nHence, the transition should be smooth and natural. Avoid abrupt transitions.\nMake sure the first to speak is different from the previous speaker. Look at the last tag in CONTEXT to determine the previous speaker. \nIf last tag in CONTEXT is <Person1>, then the first to speak now should be <Person2>.\nIf last tag in CONTEXT is <Person2>, then the first to speak now should be <Person1>.\nThis is a live conversation without any breaks.\nHence, avoid statements such as \"we'll discuss after a short break. Stay tuned\" or \"Okay, so, picking up where we left off\".\n`;\n\nconst podcast_name = input?.title || '<Figure podcast_name out from content>'\nconst podcast_tagline = input?.subTitle || '<Figure podcast_tagline out from content>'\nlet instruction = \"\"\n\nif (partIdx === 0) {\n    instruction = `\nALWAYS START THE CONVERSATION GREETING THE AUDIENCE: Welcome to ${podcast_name} - ${podcast_tagline}.\nYou are generating the Introduction part of a long podcast conversation.\nDon't cover any topics yet, just introduce yourself and the topic. Leave the rest for later parts, don't include closing statements, following these guidelines:\n    `;\n} else if (partIdx === totalParts - 1) {\n    instruction = `\nYou are generating the last part of a long podcast conversation. \n${COMMON_INSTRUCTIONS}\nFor this part, discuss the below INPUT and then make concluding remarks in a podcast conversation format and END THE CONVERSATION GREETING THE AUDIENCE WITH PERSON1 ALSO SAYING A GOOD BYE MESSAGE, following these guidelines:\n    `;\n} else {\n    instruction = `\nYou are generating part ${partIdx + 1} of ${totalParts} parts of a long podcast conversation.\n${COMMON_INSTRUCTIONS}\nFor this part, discuss the below INPUT in a podcast conversation format, following these guidelines:\n    `;\n}\n\nconst params = {\n  roles_person1: \"Host\",\n  roles_person2: \"Researcher/Expert\",\n  conversation_style: \"engaging, fast-paced, enthusiastic, educational, conversational, precise, professional\",\n  dialogue_structure: input?.dialogue_structure ?? \"Paper Overview, Problem Statement, Prior Work, Methodology, Key Findings, Critical Analysis, Practical Impact, Future Directions, Key Takeaways\",\n  engagement_techniques: \"rhetorical_questions, analogies, real_world_examples, storytelling, thought_experiments, humor, counterpoints, audience_challenges, emotional_hooks, transition_phrases\",\n  input_text: JSON.stringify(chunk),\n  podcast_name: podcast_name,\n  podcast_tagline: podcast_tagline,\n  instruction: instruction,\n  user_instructions: input?.user_instruction ?? '',\n  output_language: input?.language ?? \"English\"\n};\n\nreturn params;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1240,
        -200
      ],
      "id": "69521b0a-eb6b-4d29-84bf-762ebba9552d",
      "name": "Configs"
    },
    {
      "parameters": {
        "options": {
          "reset": false
        }
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        -1020,
        -200
      ],
      "id": "d931da23-54d9-475c-a784-b4d8a0b95dc3",
      "name": "Chunks Looper",
      "executeOnce": false
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "let previousResponse = null;\nlet previousContext = null\n\nif ($runIndex > 0) {\n  previousResponse = $('Basic LLM Chain').first().json;\n  previousContext = $('Loop Setup').first(0, $runIndex - 1).json.context\n}\n\nif ($runIndex == 0) {\n    $input.item.json.context = $json.input_text\n} else if ($runIndex == 1) {\n    $input.item.json.context = previousResponse.text\n} else {\n    $input.item.json.context = previousContext + previousResponse.text\n}\n\n$input.item.json.partIdx = $runIndex + 1\n$input.item.json.totalParts = $('LangChain Splitter').all().length\n\nreturn $input.item;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -740,
        -160
      ],
      "id": "42028045-acd3-4d55-a122-1e69e84232ea",
      "name": "Loop Setup"
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash-exp",
        "options": {
          "temperature": 0.7
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [
        -340,
        60
      ],
      "id": "1b49a0f7-25f2-4a8f-be3d-40db82f8a4bf",
      "name": "Google Gemini Chat Model",
      "credentials": {
        "googlePalmApi": {
          "id": "3pnf66IfJaRkSHiT",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://texttospeech.googleapis.com/v1beta1/text:synthesize",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "googlePalmApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"input\": {\n    \"text\": {{ JSON.stringify($json.content) }}\n  },\n  \"audioConfig\": {\n    \"audioEncoding\": \"MP3\"\n  },\n  \"voice\": {\n    \"languageCode\": \"en-US\",\n    \"name\": \"{{ $json.role == \"person1\" ? \"en-US-Journey-D\" : \"en-US-Journey-O\"; }}\"\n  } \n}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -480,
        -760
      ],
      "id": "a516b8e6-bccf-43c2-947f-8f9dc99edd7f",
      "name": "HTTP Request",
      "retryOnFail": true,
      "maxTries": 5,
      "waitBetweenTries": 5000,
      "executeOnce": true,
      "credentials": {
        "googlePalmApi": {
          "id": "3pnf66IfJaRkSHiT",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "operation": "toBinary",
        "sourceProperty": "audioContent",
        "options": {}
      },
      "type": "n8n-nodes-base.convertToFile",
      "typeVersion": 1.1,
      "position": [
        -260,
        -760
      ],
      "id": "89569bf9-9796-4359-b658-eb733a404900",
      "name": "Convert to File"
    }
  ],
  "pinData": {
    "Execute Workflow Trigger": [
      {
        "json": {
          "author": "Test author",
          "cover": "https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41746-024-01356-6/MediaObjects/41746_2024_1356_Fig1_HTML.png",
          "language": "English",
          "user_instruction": "",
          "title": "HuatuoGPT-o1, Towards Medical Complex Reasoningwith LLMs",
          "content": "HuatuoGPT-o1, Towards Medical Complex Reasoning\nwith LLMs\nJunying Chen\n1\n, Zhenyang Cai\n1\n, Ke Ji\n1\n, Xidong Wang\n1\n, Wanlong Liu\n1\nRongsheng Wang\n1\n, Jianye Hou\n1\n, Benyou Wang\n1,2∗\n1 \nThe Chinese University of Hong Kong, Shenzhen\n2 \nShenzhen Research Institute of Big Data\nhttps://github.com/FreedomIntelligence/HuatuoGPT-o1\nAbstract\nThe breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to\nimprove LLM. Yet, most research in reasoning has focused on mathematical tasks,\nleaving domains like medicine underexplored. The medical domain, though dis-\ntinct from mathematics, also demands robust reasoning to provide reliable answers,\ngiven the high standards of healthcare. However, verifying medical reasoning is\nchallenging, unlike those in mathematics. To address this, we propose verifiable\nmedical problems with a medical verifier to check the correctness of model out-\nputs. This verifiable nature enables advancements in medical reasoning through\na two-stage approach: (1) using the verifier to guide the search for a complex\nreasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning\n(RL) with verifier-based rewards to enhance complex reasoning further. Finally, we\nintroduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which\noutperforms general and medical-specific baselines using only 40K verifiable prob-\nlems. Experiments show complex reasoning improves medical problem-solving\nand benefits more from RL. We hope our approach inspires advancements in\nreasoning across medical and other specialized domains.\n1 Introduction\nThe release of OpenAI o1 has marked a significant milestone in large language model (LLM)\ndevelopment, showcasing impressive capabilities [1 –3 ]. This breakthrough highlights the potential of\nscaling Chain-of-Thought (CoT) and reinforcement learning to enhance LLM performance [ 4–6 ].\nWhile subsequent research efforts attempt to replicate these advancements, they often remain limited\nto mathematical reasoning tasks [ 7 – 9, 6]. The application of o1-like methods to specialized fields,\nsuch as medicine, remains largely underexplored.\nMedical tasks often involve complex reasoning [ 10– 12 ]. In real-world medical diagnoses or deci-\nsions, doctors often deliberate carefully. Such life-critical field necessitates meticulous thinking to\nensure more reliable answers [13 , 14 ]. Additionally, the medical domain offers unique advantages:\ncompared to general domains, the medical domain is generally narrower in scope and easier to verify.\nFurthermore, medical reasoning closely resembles real-world applications in fields like finance, law,\neducation, and security, making advancements in this area readily transferable [15, 16].\nDespite these advantages, a key challenge in medical reasoning is verifying the thought process,\nwhich often lacks clear steps. Inspired by mathematical problems that allow verification through their\noutcomes, we construct 40K verifiable medical problems reformatted from challenging, closed-set\nmedical exam questions. These verifiable problems are characterized as open-ended with unique,\n∗\nBenyou is the corresponding author with email: wangbenyou@cuhk.edu.cn\nPreprint. Under review.\narXiv:2412.18925v1 [cs.CL] 25 Dec 2024\n\nobjective ground-truth answers that allow an LLM verifier to check solution correctness. This enables\na two-stage approach for advancing medical complex reasoning:\nStage 1: Learning Complex Reasoning We construct complex reasoning trajectories through\nstrategy-based searches guided by verifier feedback (True or False). The LLM first initializes a CoT.\nIf the verifier rejects the current CoT, the model extends the CoT by applying a strategy sampled from\nBacktracking, Exploring New Paths, Verification, and Correction until a correct answer is provided.\nSuccessful reasoning trajectories are then used to fine-tune the LLM, enabling it develop complex\nreasoning skills that embody iterative reflection.\nStage 2: Enhancing Complex Reasoning with RL After acquiring complex reasoning skills,\nreinforcement learning (RL) further refine this ability. Specifically, sparse rewards provided by the\nverifier guide self-improvement using the Proximal Policy Optimization (PPO) algorithm.\nUsing this approach, we present HuatuoGPT-o1, a medical LLM capable of producing a long CoT to\nrecognize its mistakes, try different strategies and refine the answer. Experiments demonstrate that our\nmethod (using only 40K data points) yields an 8.5-point improvement on medical benchmarks with an\n8B model. Furthermore, our 70B model outperforms other open-source general and medical-specific\nLLMs across multiple medical benchmarks. The experiments further reveal that complex reasoning\nenhances medical problem-solving and boosts RL performance compared to standard or non-CoT\nmethods. Our contributions are as follows:\n• To the best of our knowledge, this is the first work to advance medical complex reasoning in\nLLMs using verifiable medical problems and a medical verifier.\n• With verifiable medical problems, we propose a two-stage training approach, combining search\nstrategies to construct reasoning pathways for fine-tuning, and further enhanced by RL with\nverifier feedback.\n• \nUsing the proposed method, we developed HuatuoGPT-o1, the first medical LLM capable of\ncomplex reasoning. HuatuoGPT-o1 exhibits superior performance compared to the open-source\ngeneral and medical-specific baselines.\n• \nOur experiments reveal that complex reasoning is effective for medical problem-solving and\nbenefits RL enhancements.\n2 Verifiable Medical Problems\nVerifiable Medical Problem (x): A 30-year-old woman recently\ntraveled to India and now presents with shaking, chills, fevers, headaches,\npallor, and jaundiced sclera. Vital signs: Temp 38.9°C, RR 19/min, BP 120/80\nmm Hg, Pulse 94/min. Labs: Hct 30%, Total bilirubin 2.6 mg/dL, Direct\nbilirubin 0.3 mg/dL. What is the most severe complication of this condition?\nx\nGround-true Answer (y*): Cerebral edema\ny*\nExam Question: A 30-year-old woman recently traveled to India and now presents with\nshaking, chills, fevers, headaches, pallor, and jaundiced sclera. Vital signs: Temp 38.9°C,\nRR 19/min, BP 120/80 mm Hg, Pulse 94/min. Labs: Hct 30%, Total bilirubin 2.6 mg/dL,\nDirect bilirubin 0.3 mg/dL. What is the most severe complication of this condition?\nOptions: A. Heart block B. Facial paralysis C. Cerebral edema D. Aplastic crisis\nExam Answer: C\nSelect suitable and\nchallenging questions\nx \ny*\nVerifier\n(2) Medical Verifier\n(1) Verifiable Medical Problems\nClose-set Questions\n(MedQA & MedMCQA\nTraining Set)\nchallenging\nClose-set Questions\nx\ny*\nVerifiable Problem\n(e , y)\nVerify whether the\nreasoning is correct.\nLLM\ny == y*?\nCoT Answer\nFilter and\nTransfer\nFigure 1: Left: Constructing verifiable medical problems using challenging close-set exam questions.\nRight: The verifier checks the model’s answer against the ground-truth answer.\nInspired by mathematical problems that enable verification of the solution process through the\nfinal result, we aim to create verifiable medical problems that allow reasoning verification through\noutcomes. These verifiable problems are characterized as open-formal with unique, objective ground-\ntruth answers, as illustrated in Figure 1.\nSourcing from Medical Exam Questions To achieve this, we utilize closed-set real-world exam\nquestions for two key reasons: 1) a large number of medical exam questions are available; and 2)\nthese exam questions are typically objective and accurate. Specifically, we collected 192K medical\nmultiple-choice exam questions from the training sets of MedQA-USMLE [17] and MedMcQA [18].\n2\n\nTransforming to Verifiable Medical Problems However, these medical questions are closed-set,\nmeaning they provide limited options to choose from. This makes it easy for models to guess the\ncorrect answer without proper reasoning. Additionally, some questions are not suitable due to they\nmay lack a unique correct answer for verification or are too simple to require reasoning.\nTo address this, we select and process the questions as follows:\n1. \nSelecting Challenging Questions We removed questions that three small LLMs (Gemma2-\n9B [19 ], LLaMA-3.1-8B [20 ], Qwen2.5-7B [21 ]) all answered correctly and discarded short\nquestions to retain those requiring deeper reasoning.\n2. Ensure Unique Answers: We excluded questions asking for “incorrect options” or with multiple\ncorrect answers. A LLM (GPT-4o) is further employed to remove questions where the correct\nanswer might not be unique or could be ambiguous.\n3. \nReformatting to Open-Ended Formal: Using LLMs (GPT-4o), We reformatted each closed-set\nquestion into open-ended problem an open-ended problem x and a ground-truth answer y\n∗\n, as\nshown in Figure 1.\nThe prompt used for filtering and processing can be found in Appendix B. After this filtering and\nprocessing, we ultimately constructed a dataset of 40K verifiable medical questions denoted as\nD = {(x, y\n∗\n)}, where x is a verifiable problem and y\n∗ \nthe ground-truth answer.\nDeveloping Medical Verifier With these verifiable problems, we propose a verifier to assess the\ncorrectness of model outputs. Given a medical verifiable problem x, the model generates a Chain-\nof-Thought (CoT) e and a result y. The verifier checks y against the ground-truth answer y\n∗ \nand\nprovides binary feedback as:\nVerifier(y, y\n∗\n) ∈ {True, False}\nThis feedback is essential for building a correct reasoning trajectory and improving reasoning\nperformance. We use GPT-4o [ 22 ] as the verifier, prompting it to perform validation with the detailed\nprompt provided in Appendix C. Given the prevalence of aliases in the medical domain, exact match\nmethods [8, 23] commonly applied in mathematics are impractical. Experiments in Section 4.2\nconfirm this and demonstrate the reliability of the LLM-based verifier.\n3 Methodology\nx\nVerifiable Medical Problem\nVerifier\nBacktracking \nExploring New\nPaths\nVerification Correction\ny*\nGround-true Answe\nVerify\nStage 1. Learning Complex Reasoning \n## Thinking\nComplex CoT ( )\n## Response\nResponse ( )\nStage 2. Enhance Complex\nReasoning with RL\nVerify false, sample saech\nstrategy to refine result\nVerify true,\nStop\nVerify false and i > N,\nrestart the search.\nOnpolicy Learning\n(PPO)\nStage1\nSFT\nx\nVerifier\nStage2\nRL\n...\ny*\nTransfer\n[e\n0\n,y\n0\n,...,e\ni\n,y\ni\n] to\nComplex CoT\nFormal Response\nVerifiable Medical Problem \nGround-true Answe\nReward\nTrained LLM\nOkay, let's think about maintaining t...\nSo, when placing the finish line of a ...\nBut let me pause here. I've read that ...\nInstead, it might actually make more ...\nThen again, it's also important to revi...\nWhen I really think about it, there’s ...\nAll these considerations make me ....\nWait, actually, this may not help us ...\n...\nWhew, after going through that, it seems\nlike cerebral edema, often linked ...\nThe clinical presentation and laboratory\nfindings suggest the patient is suffering\nfrom malaria ...\nThe most severe complication that can\narise from this particular condition is\ncerebral malaria ...\nFigure 2: Demonstration of developing and improving LLMs for medical complex reasoning. Left\n(Stage1): Searching for correct reasoning trajectories to fine-tune LLMs for complex reasoning.\nRight (Stage2): Using the verifier to enhance complex reasoning via reinforcement learning.\nIn this section, we present the method for training LLMs to performing medical complex reasoning\nto identify errors, and refine answers using deep thinking. As shown in Figure 1, the method has two\nstages: Stage One: master complex reasoning, and Stage Two, enhance complex reasoning with\nreinforcement learning (RL).\n3\n\nVerifiable Medical Problem (x) A 51-year-old man experiences progressively worsening dyspnea on exertion and fatigue over two months.\nExamination and test results include cardiac exam without murmurs, coarse crackles at lung bases, an ECG with an irregularly irregular rhythm and\nabsent p waves, a chest x-ray showing globular cardiac shadow enlargement with prominent hila and bilateral fluffy infiltrates, and echocardiography\nindicating a dilated left ventricle with an ejection fraction of 40%. What is the most likely cause of this patient's cardiac condition?\nGround-true Answer (y*): Chronic supraventricular tachycardia\nOur Model Response ( , )\nInit CoT ( e\n0 \n, y\n0 \n)\n## CoT ( e\n0 \n)\nIdentify Key Clinical Features: The 51-year-old man is experiencing ...\nAnalyze Chest X-ray Findings: The chest x-ray shows a globular cardiac ...\nEvaluate Potential Causes of Heart Failure: Heart failure in this context can ...\n...\n## Conclusion (y\n0\n)\nThe most likely cause of this patient's cardiac condition is alcohol-induced dilated\ncardiomyopathy.\nSearch with Verification Strategy ( e\n1 \n, y\n1 \n)\n## CoT ( e\n1 \n)\nRe-evaluate Key Clinical Features: Re-examining the patient's symptoms, we ...\nAssess Chest X-ray and Pulmonary Findings: The chest x-ray findings ...\n...\n## Conclusion (y\n1\n)\nAlthough alcohol-induced cardiomyopathy was initially considered, ischemic\ncardiomyopathy is more likely the cause of this patient's condition ...\nSearch with Correction Strategy ( e\n2 \n, y\n2 \n)\n## CoT ( e\n2 \n)\nReassess the Potential Causes of Dilated Cardiomyopathy: Re-examining the ...\nFocus on Patient History and Lifestyle: Explore elements from the patient's ..\n...\n## Conclusion (y\n2\n)\nThe most likely cause of this patient's cardiac condition is alcoholic cardiomyopathy...\nSearch with Exploring New Paths Strategy ( e\n3 \n, y\n3 \n)\n## CoT ( e\n3 \n)\nConsider Effects of Atrial Fibrillation: Atrial fibrillation can lead to a reduced ...\nReview Cardiac Presentation: Globular cardiac shadow and absence of ...\n...\n## Conclusion (y\n3\n)\n... the most likely cause of the patient's cardiac condition is chronic supraventricular\ntachycardia (atrial fibrillation)-induced cardiomyopathy.\n## Thinking ( )\nHmm, let's look at the clinical picture here. We have a 51-year-old man with\nprogressively worsening dyspnea, fatigue, an irregularly irregular heart ...\n...\nNow, let’s think about dilated cardiomyopathy. The combination of a ...The most likely\ncause of this patient's condition is alcohol-induced dilated cardiomyopathy ...\nBut wait — let's pause and re-evaluate this. Are we jumping to conclusions here?\nLet's go over the symptoms and findings again to see if we missed anything ...\n...\nCould there be other causes of dilated cardiomyopathy? Viral myocarditis, genetic\nfactors, or nutritional deficiencies could also be responsible ... Maybe ischemic\ncardiomyopathy is a better explanation.\nBut hold on — the patient hasn’t mentioned typical angina symptoms, and there's\nno clear evidence of coronary artery disease in the findings provided. Hmm ...\nThen again, the globular cardiac shadow on the chest x-ray is interesting ... Hmm,\nthis supports a non-ischemic etiology.\n...\nAlright, back to alcohol-induced cardiomyopathy. It fits the demographic and clinical\npicture, even if we don’t have explicit evidence of alcohol use.\nNot so fast, what if the atrial fibrillation itself is the cause? Chronic atrial fibrillation\ncan lead to tachycardia-induced cardiomyopathy ...\n...\nHmm, let’s connect the dots. Persistent high heart rates from atrial fibrillation could\nexplain the dilated left ventricle and reduced ejection fraction.... Maybe the primary\nissue here is tachycardia-induced cardiomyopathy due to chronic atrial fibrillation ...\n...\nYes, I think this is it. The most likely diagnosis is chronic supraventricular\ntachycardia-induced cardiomyopathy.\n## Response ( )\nThe clinical presentation and investigation findings suggest that this 51-year-old man is\nexperiencing heart failure with reduced ejection fraction, likely due to tachycardia-induced\ncardiomyopathy. His irregularly irregular ECG rhythm with absent p waves indicates chronic ...\nStream of Search (Total 1565 tokens)\nstop\nComplex CoT (758 tokens) & Response (155 tokens)\nFigure 3: Example of data synthesis. Left: strategy search on medical verifiable problems until\nthe answer is validated. Right: Merging the entire search process into efficient complex CoTs,\nfacilitating effective deep reasoning to refine answers. The complex CoTs and responses are used to\ntrain the model to adopt thinks-before-it-answers behavior akin to o1.\n3.1 Stage One: Learning Complex Reasoning\nSearching for Correct Trajectories Given a verifiable medical problem as a tuple (x, y\n∗\n), i.e.\n(question, ground-true answer), the LLM (e.g., GPT-4o) generates an initial CoT e\n0 \nand answer y\n0\n:\ne\n0\n, y\n0 \n= LLM\ninit\n(x)\nThe verifier checks if y\n0 \nmatches y\n∗\n. If incorrect, the model iteratively refines the answer by applying\na randomly selected search strategy k ∈ K on prior thoughts [e\n0\n, y\n0\n, . . . , e\ni−1\n, y\ni−1\n], producing new\nreasoning e\ni \nand new answer y\ni\n:\ne\ni\n, y\ni \n= LLM\nk\ni \n(x, [e\n0\n, y\n0\n, . . . , e\ni−1\n, y\ni−1\n])\nwhere i denotes the i-th iteration. We define four search strategies K to guide the refinement process:\n• Exploring New Paths The LLM explores a new approach e\ni \n, distinct from prior e\n0\n, . . . , e\ni−1\n, to\nderive a new answer y\ni\n.\n• Backtracking The LLM revisits a previous reasoning process e\nj \n, y\nj \n, where j < i − 1, and\ncontinues reasoning from there. Note that Backtracking is sampled only if i ≤ 2.\n• Verification The LLM evaluates the current reasoning e\ni−1\nand result y\ni−1\n, providing a validation\nprocess e\ni \nand the verified result y\ni\n.\n• Corrections The LLM critiques and corrects the current reasoning e\ni−1\n, yielding a revised\nreasoning e\nj \nand answer y\ni\n.\nThe process iterates until y\ni \nis verified as correct. If the maximum iteration count N = 3 are reached,\nthe search restarts. Each data point (x, y\n∗\n) is given up to T = 3 attempts; if all fail, the data point is\ndiscarded. The prompts for search reasoning trajectories can be found in Appendix D.\nConstructing SFT Training Data When a successful trajectory [e\n0\n, y\n0\n, . . . , e\ni\n, y\ni\n] is found, it is\nreformatted into a coherent, natural language reasoning process ˆe (Complex CoT):\n4\n\nAlgorithm 1: Training LLMs for Medical Complex Reasoning\nRequire: Medical Verifiable Problems D = {(x, y\n∗\n)}, a Verifier, an LLM (GPT-4o) for synthesizing\nreasoning trajectories, search strategies K, max search depth N , max search attempts T , and initial policy\nπ\nθ \n.\nD\nSearch\n, D\nRL \n← Split(D)\nD\nSFT \n← ∅\n// Stage One: Learning Complex Reasoning\nfor (x, y\n∗\n) ∈ D\nSearch \ndo\nfor j ← 1 to T do\ne\n0\n, y\n0 \n← LLM\ninit\n(x)\nfor i ← 1 to N do\nk\ni \n∼ K\ne\ni\n, y\ni \n← LLM\nk\ni \n(x, [e\n0\n, y\n0\n, ..., e\ni−1\n, y\ni−1\n])\nif Verifier(y\ni\n, y\n∗\n) then\nˆe ← LLM\nReformat\n([e\n0\n, y\n0\n, ..., e\ni\n, y\ni\n])\nˆy ← LLM\nResponse\n(ˆe)\nD\nSFT \n← D\nSFT \n∪ {(x, ˆe, ˆy)}\nbreak\nif Verifier(y\ni\n, y\n∗\n) then\nbreak\n// SFT\nfor (x, ˆe, ˆy) ∈ D\nSF T \ndo\nL\nSFT\n(θ) ← − log π\nθ \n(ˆe, ˆy | x)\nθ ← UpdateParameters(L\nSF T \n(θ), θ)\n// Stage Two: Enhance Reasoning with RL\nπ\nref \n← π\nθ\nfor (x, y\n∗\n) ∈ D\nRL \ndo\nˆe, ˆy ∼ π\nθ \n(x)\n// Reward\nr ← Rule (Verifier (ˆy, y\n∗\n)) − βKL (π\nθ \n(· | x) || π\nref \n(· | x))\nθ ← UpdateParameters (L\nRL \n(x, ˆe, ˆy, r, π\nref \n, π\nθ \n) , θ)\nreturn π\nθ\nˆe = LLM\nReformat\n([e\n0\n, e\n1\n, . . . , e\ni\n, y\ni\n])\nAs shown in Figure 3, this reformatting avoids rigid structures, using smooth transitions (e.g., “hmm,”\n“also,” “wait”) to streamline reasoning and reduce token usage.The model then generates a formal\nresponse ˆy for question x using the conclusion of ˆe:\nˆy = LLM\nResponse\n(x, ˆe)\nThe prompt used for constructing SFT data can be found in Appendix E.\nSupervised Fine-Tuning (SFT) We synthesize 20K SFT data points D\nSFT \n= {(x, ˆe, ˆy)} from the\nverifiable problem set D = {(x, y\n∗\n)} using GPT-4o. D\nSFT \nis used to fine-tune LLMs to generate a\ncomplex CoT ˆe followed by a formal response ˆy. This fine-tuning process teaches the model to think\nbefore answering, encouraging a Stream-of-Search (SoS) [ 23] way where the model deeply explores\nand refines its reasoning before answering.\n3.2 Stage Two: Enhance Complex Reasoning with RL\nIn this stage, we further enhance the complex reasoning skills using reinforcement learning (RL).\nWhile the LLM learned successful reasoning trajectories in stage 1, these paths, derived via search,\nmay not be optimal. On-policy learning in stage 2 aims to refine the model for better complex CoT\nreasoning.\n5\n\nRewards of RL Rewards play a crucial role in guiding the RL training target. Given a verifiable\nproblem x and the generated response (ˆe, ˆy), the reward is assigned as:\nr\n′\n(x, ˆy, y\n∗\n) =\n\n\n\n1 if verifier(ˆy, y\n∗\n) = True\n0.1 if verifier(ˆy, y\n∗\n) = False\n0 if ˆy = null\nFollowing [ 24 , 25, 8 ], correct answers receive a reward of 1, incorrect answers receive 0.1, and\nresponses that lack think-before-answering behavior receive 0. Additionally, following related works,\nthe total reward combines this function score with the Kullback-Leibler (KL) divergence between the\nlearned RL policy π\nθ \nand the initial policy π\nref\n, scaled by a coefficient β:\nr(x, ˆy, y\n∗\n) = r\n′\n(x, ˆy, y\n∗\n) + βKL(θ)\nto stabilize training with sparse rewards [8].\nReinforcement Learning For RL, We use the Proximal Policy Optimization (PPO) [ 26] algorithm\nwith a clipped objective. The fine-tuned model serves as the policy model π\nθ \n. Training is conducted\non the remaining verifiable medical problems D\nRL \n= {(x, y\n∗\n)}. The policy samples responses (ˆe, ˆy)\nfor input x, computes the reward, and updates parameters θ.\nThe full training process for both stages is summarized in Algorithm 1.\n4 Experiments\n4.1 Experimental Setup\nTraining Data Finally, We constructed a 40K medical verification dataset D = {(x, y\n∗\n)} from the\ntraining sets of MedQA-USMLE [ 17 ] and MedMCQA [ 27]. Of this, 20K is used for SFT in stage\n1 and 20K for RL in stage 2. Additionally, 4K unconverted data (close-set questions with option\nanswers) from D are included to enhance generalization. In line with prior work that integrates\ngeneral-domain data to support medical adaptation [15, 28], we add 5K general verification questions\nsourced from MMLU-Pro [29 ] outside the medical-related tracks. All data were strictly screened to\navoid contamination with the evaluation data using the filtering method of Med-PaLM2 [ 30 ] (filtering\noverlaps of 64 consecutive characters).\nModel Training Using the proposed method, we train our models HuatuoGPT-o1-8B and\nHuatuoGPT-o1-70B based on LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct [20], respec-\ntively. In Stage 1, the models are fine-tuned on the D\nSFT \nfor 3 epochs with a learning rate of 5e-6 and\na batch size of 128. In Stage 2, we employ PPO for RL with a learning rate of 5e-7, a batch size of\n128, and β set to 0.03. The PPO parameters are set as: 3 PPO epochs, a discount factor 1.0, a value\ncoefficient 1.0, and a clip range 0.2.\nBaselines We compare our models with two type of LLMs: 1) General LLMs: Qwen-2.5 [31 ],\nLLaMA-3.1 [20 ], Gemma 2 [19 ], Yi [32 ], Mistral [33 ]; and 2) Medical-Specific LLMs: UltraMedical\n[28], OpenBioLLM [34], and BioMistral [35].\nBenchmarks We evaluate on standard medical benchmarks: MedQA (USMLE test set) [17 ],\nMedMCQA (validation set) [18 ], and PubMedQA (test set) [ 36]. Aditionally, we evaluated the\nmedical sections of some challenging LLM benchmarks, including the health and biology tracks of\nMMLU-Pro [29 ], and the genetics and molecular biology tracks of GPQA [37 ]. Due to the limited\nnumber of GPQA questions, we ran this evaluation 5 times and averaged the results.\n4.2 Experimental Results\nMain Results We evaluated various open-source LLMs on medical tasks, as shown in Table 1. The\nresults indicate that prior medical-specific LLMs, like UltraMedical, excel on traditional medical\nbenchmarks (MedQA, MedMCQA, PubMedQA) but struggle on the newer, more challenging datasets,\n6\n\nMedQA \nMedMCQA PubMedQA\nMMLU-Pro GPQA\nAvg.\nHealth Biology Genetics \nMolecular\nBiology\n∼ 8B Large Language Models\nBioMistral-7B 45.0 40.2 66.9 27.4 49.2 28.6 38.5 42.3\nOpenBioLLM-8B 57.7 54.1 74.1 38.4 52.4 43.7 39.6 51.4\nUltraMedical-8B 71.1 58.3 77.4 55.1 66.7 41.2 48.4 59.7\nMistral-7B-Instruct 48.2 44.6 59.5 33.7 53.6 30.0 46.1 \n45.1\nYi-1.5-9B-Chat 50.8 48.7 69.8 43.4 65.6 42.5 48.1 52.7\nLLaMA-3.1-8B-Instruct 58.7 56.0 75.2 52.7 64.6 33.8 46.8 55.4\nGLM-4-9B-Chat 58.9 49.8 73.5 45.5 65.4 53.8 41.6 55.5\nQwen2.5-7B-Instruct 57.0 55.6 72.7 50.6 70.2 \n36.2 49.7 56.0\nGemma2-9B 61.8 55.9 63.3 55.1 74.9 35.0 57.4 57.6\nHuatuoGPT-o1-8B 72.6 60.4 79.2 58.7 68.2 48.8 59.7 63.9\nw/o Stage2 (RL) 69.0 57.9 77.7 53.5 66.1 41.2 53.5 59.8\n> 10B Large Language Models\nUltraMedical-70B 82.2 71.8 78.4 64.8 71.1 33.8 62.9 66.4\nOpenBioLLM-70B 76.1 74.7 79.2 68.8 76.7 38.8 54.8 67.0\nDeepSeek-67B-Chat 57.1 51.7 76.1 46.9 66.2 40.0 51.0 55.6\nYi-1.5-34B-Chat 59.5 56.7 74.3 52.8 71.0 32.5 56.8 57.7\nGemma2-27B 65.4 60.2 72.6 61.1 76.2 32.5 61.6 \n61.4\nQwen2.5-72B-Instruct 72.7 66.2 71.7 65.3 78.8 41.2 56.8 64.7\nQwQ-32B-Preview 72.3 65.6 73.7 62.0 78.1 37.5 64.5 64.8\nLlama-3.1-70B-Instruct 78.4 72.5 78.5 68.2 80.8 52.5 61.6 70.3\nHuatuoGPT-o1-70B 83.3 73.6 80.6 71.0 82.8 56.2 66.5 73.4\nw/o Stage2 (RL) 80.3 70.1 78.6 70.2 79.8 54.2 63.9 71.0\nTable 1: Main Results on Medical Benchmarks. LLMs with are specifically trained for the medical\ndomain, and indicates LLMs training for long chain-of-thought reasoning. \"w/o\" means \"without\".\nWithin each segment, bold highlights the best scores, and underlines indicate the second-best.\neven when the questions are medically related. This may suggest that MMLU-Pro and GPQA require\nnot only medical knowledge but also stronger reasoning capabilities.\nOur model, HuatuoGPT-o1, performs exceptionally across all datasets. The 8B version outperforms\nthe base model (LLaMA-3.1-8B-Instruct) by 8 points in overall evaluation. Furthermore, our 70B\nmodel surpasses other comparable open-source LLMS, including QwQ-32B, which are also developed\nspecifically for advanced reasoning capabilities. These results demonstrate the effectiveness of our\napproach. Additionally, compared to only fine-tuning (w/o RL), the two-stage training strategy\nsignificantly improves performance, benefiting from the verifiable medical problems.\nAblation Study We conducted an ablation study on the 8B model to analyze the impact of Complex-\nCoT and RL The results, shown in Table 2, reveal the following insights:\n1. Simple Multiple-Choice Training Is Ineffective: We compared the performance of models\ntrained solely on the original medical multiple-choice questions of dataset D. Specifically, we used\nmultiple-choice questions as inputs and the correct option as output for fine-tuning. The results\nindicate that raining solely on multiple-choice questions (the fine-tuned baseline) yields minimal\nimprovement over the base model (LLaMA-3.1-8B-Instruct). This suggests that learning correct\nanswers alone does not improve problem-solving ability.\n2. Effectiveness of Complex CoTs: We further examined the impact of different types of Chain-of-\nThought (CoT) reasoning. The results show that direct learning of response (ˆy) performs the worst,\nwhile simple CoT (y\n0\n, e\n0\n) offers only little benefit. In contrast, Complex CoT (ˆy\n0\n, ˆe) significantly\nimproves performance by an average of 4.3 points. This demonstrates the importance of teaching\nmodels to refine their answers with reflection.\n3. Complex CoT Boosts RL: We compared the RL enhancements under different CoT strategies,\nas shown in Table 3. The results indicate that Complex CoT, which involves much longer CoT (an\naverage of 712 tokens), yields a significantly greater gain (3.6 points) compared to simple CoT (2.6\npoints) and no CoT (1.1 points), as detailed in Table 3. This may suggests that longer self-play\nreasoning paths provide richer thought processes and feedback, enabling the model to discover\nhigher-reward solutions.\n7\n\nMedQA MedMCQA PubMedQA \nMMLU-Pro\n(Med )\nGPQA\n(Med )\nBaseline LLMs\nLLaMA-3.1-8B-Instruct 58.7 56.0 75.2 58.2 44.1\nFine-Tuned Baseline\nSFT w/ Original Exam Data of D 60.0 55.5 74.1 54.3 46.9\nEffectiveness of Complex Chain-of-Thought (CoT)\nSFT w/o \n\u0018\u0018\nCoT (only ˆy) 65.2 58.1 75.4 58.5 48.7\nSFT w/ Simple CoT (x\n0\n, y\n0\n) 66.6 59.2 75.4 57.0 46.7\nSFT w/ Complex CoT (ˆx, ˆy) 69.0 57.9 77.7 59.4 51.0\nEffectiveness of RL\nSFT w/o \n\u0018\u0018\nCoT + RL w/ PPO 66.4 58.6 76.3 60.1 49.8\nSFT w/ Simple CoT + RL w/ PPO 68.7 58.4 77.5 60.2 53.1\nSFT w/ Complex CoT + RL w/ PPO 72.6 60.4 79.2 63.1 57.5\nComparison of Different RL Algorithms\nSFT w/ Complex CoT + RL w/ DPO 72.2 58.4 77.3 60.4 52.5\nSFT w/ Complex CoT + RL w/ RLOO 71.1 60.1 78.1 60.9 58.2\nSFT w/ Complex CoT + RL w/ PPO 72.6 60.4 79.2 63.1 57.5\nTable 2: The results of ablation experiments on HuatuoHPT-o1-8B. (Med ) indicates that only the\nmedical-related parts are evaluated. \"w/o\" and \"w/\" denote \"without\" and \"with\". \"Original Exam\nData\" refers to original multiple-choice questions used for medical verifiable problems D. Bold\nhighlights the best scores in each segment.\n4. PPO Yields the Best Performance: Using the same reward function, we further compared\ndifferent RL-related algorithms, including the preference learning algorithm DPO [38] and the\nREINFORCE-style algorithm RLOO [ 39 ]. Detailed implementation information is provided in\nAppendix F. Comparing PPO, RLOO, and DPO, we find PPO performs best, followed by RLOO and\nDPO. The weaker performance of DPO likely results from its off-policy nature, while PPO benefits\nfrom its use of value models, despite higher memory consumption.\n# Avg. Generated\nTokens\n∆ Avg. Gain\nfrom RL\nDirect Response (only ˆy) 82 1.1\nSimple CoT (x\n0\n, y\n0\n) 281 2.6\nComplex CoT (ˆx, ˆy) 712 3.6\nTable 3: Comparison of models trained with different reasoning strategies. \"# Avg. Tokens\" indicates\nthe average number of tokens generated per question. ∆ represents the performance improvement\nfrom RL, as detailed in Table 1.\nReliability of the Verifier The verifier plays a crucial role in guiding path search and reinforcement\nlearning (RL). In our approach, GPT-4o serves as the verifier to assess model outcomes against\nground-truth answers. To assess its reliability, we manually verified 200 scoring instances sampled\nfrom Stage 1 and Stage 2. As shown in Figure 4, GPT-4o achieved 96.5% accuracy in Stage 1 and\n94.5% in Stage 2, demonstrating its reliability. In contrast, the Exact Match method [ 8], which uses\nregular expressions to determine whether the correct answer is present in the response, performed\nsignificantly worse, with accuracies of only 70.5% in Stage 1 and 74.5% in Stage 2. This highlights\nthe critical role of LLM-based verifiers. Additionally, we fine-tuned an 8B verifier based on LLaMA-\n3.1-8B with 20,000 scoring samples. The fine-tuned verifier also demonstrated feasibility, achieving\nover 90% accuracy.\nDomain Compatibility To verify domain compatibility, we extra applied our method to the Chinese\nmedical domain. We constructed a dataset of 40,000 verifiable Chinese questions from the CMB-\nexam training set. We then trained HuatuoGPT-o1-7B-zh using our two-stage approach based on\nQwen2.5-7B-Instruct. As shown in Table 4, HuatuoGPT-o1-7B-zh outperformed other Chinese LLMs\n8\n\nVerifier In Stage 1 Verifier In Stage 2\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\nAccuracy (%)\n \n96.5% \n94.0%\n94.5% \n91.0%\n70.5% \n74.5%\nGPT-4O Verifier\nFine-tuned Verifier (8B)\nExact Match\nFigure 4: Accuracy of verifiers. Accuracy is based on 200 manually annotated samples.\nMedQA\n(Chinese)\nCMB\n(Exam) \nCMExam \nCMMLU\n(Med )\nHuatuoGPT2-7B 73.7 63.6 67.4 58.4\nYi-1.5-9B-Chat 75.8 66.2 68.1 64.2\nQwen2.5-7B 71.4 70.7 70.4 70.5\nGLM-4-9B-chat 75.2 70.0 70.5 67.6\nHuatuoGPT-o1-7B-zh 79.8 73.0 74.1 74.5\nw/o Stage2 (RL) 76.5 70.8 72.3 70.9\nTable 4: Results on Chinese medical benchmarks. (Med ) indicates that only the medical portion is\nevaluated. MedQA (Chinese) refers to the Chinese test set of MedQA (MedQA-MCMLE).\nof similar size, demonstrating the method’s adaptability to new domains. For more experimental\ndetails, refer to Appendix G.\n5 Related Work\nResearch on o1 Recent studies have extensively analyzed the roadmap and core techniques of\nOpenAI’s o1 [4 , 6, 5], offering foundational insights into its architecture and methodology. Extensions\nsuch as LLaMA-Berry [ 9], LLaVA-o1 [ 40 ], o1-Coder [ 41 ], and Marco-o1 [42 ] have explored o1-like\nreasoning in various domains, including mathematics, vision-language integration, and open-ended\nproblem-solving. However, these efforts have yet to address applications in medical or other highly\nspecialized fields. In contrast, research focused on medicine [2, 43 , 14 ] highlights o1’s potential\nfor deliberate, chain-of-thought reasoning in healthcare contexts. Meanwhile, several o1-inspired\nmodels, such as DeepSeek-R1-Lite-Preview [44 ], QwQ [7], and Gemini-2.0 Flash Thinking [ 45 ],\nhave emerged. Despite their promise, most of these models remain closed-source, leaving substantial\nopportunities for further exploration and application of o1’s capabilities across diverse fields.\nMedical LLMs The success of generalist LLMs has spurred interest in developing medical-specific\nLLMs to excel in the medical domain. Notably, the MedPaLM series [46 , 30] achieved over 60%\naccuracy on the MedQA benchmark, reportedly surpassing human experts. Previous medical LLMs\ntypically follow two main approaches [ 28 ]: (1) Prompting Generalist LLMs [47 , 10, 48 , 22 , 12]:\nThis method employs task-specific prompts to adapt generalist models for medical applications.\nWhile efficient and training-free, it is inherently limited by the capabilities of the original LLMs.\n(2) Further Training with Medical Data [ 49– 52 , 34, 35 , 53 – 58]: This involves training LLMs\non medical pretraining corpora or medical instructions to embed medical knowledge and expertise.\nHowever, this always requires significant computational resources, such as the 1.4 billion and 3\nbillion training tokens used for Meditron [ 59] and HuatuoGPT-II [ 15 ]. In contrast, our approach\nemphasizes enabling LLMs to excel in medical reasoning, offering a distinct solution.\nEnhancing Reasoning in LLMs Chain-of-Thought (CoT) prompting enhances the reasoning\ncapabilities of LLMs [ 60, 61 ], but scaling expert-labeled reasoning paths remains costly, especially\nfor complex problems [ 62 , 63]. To mitigate this, model-generated reasoning paths filtered through\nexternal supervision offer a partial solution [64 , 65], yet scalability challenges persist [ 66, 67 ].\n9\n\nReinforcement learning-based methods leveraging reward models or oracle functions show potential\nbut often suffer from slow processing, high costs, and supervision bottlenecks [68, 69].\nComplex Reasoning Developing models with reflective abilities like critique and self-correction\nhas shown success in reasoning, planning, and coding tasks [23 , 70– 74 ], though underexplored in\nspecialized domains like medicine. While prompting techniques can generate self-critical reasoning\n[75 , 70 ], they struggle without reliable reward functions or verifiers, particularly in complex domains\n[ 76 , 77 ]. Fine-tuning and reinforcement learning methods offer solutions but require extensive\nhuman annotations or intricate reward designs [78– 81]. Additionally, self-training methods present a\npromising direction for developing self-correction capabilities [72, 82, 83].\n6 Conclusion\nThis study advances the medical reasoning capabilities of LLMs. Firstly, we construct the medical\nverifiable problems and a medical verifier. This enabled a two-stage training process: (1) learning\ncomplex reasoning and (2) enhancing it through RL. We developed HuatuoGPT-o1, a medical LLM\nwith thinks-before-it-answers behavior, achieving outstanding performance in medical benchmarks.\nExperiments show that complex reasoning improves medical problem-solving and benefits obviously\nfrom RL. Additional validation in Chinese medical contexts shows the method’s adaptability to other\nfields. We believe our approach can enhance domain-specific reasoning beyond mathematics.\n10\n\nAcknowledgment\nThis work was supported by the Shenzhen Science and Technology Program\n(JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS20221008093330065),\nTianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC)\n(12326608), Shenzhen Key Laboratory of Cross-Modal Cognitive Computing (grant number\nZDSYS20230626091302006), and Shenzhen Stability Science Program 2023. GPU devices are all\nsupported by the university.\nReferences\n[1] \nMelody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Heylar, Rachel\nDias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes\nHeidecke, Alex Beutel, and Amelia Glaese. Deliberative alignment: Reasoning enables safer\nlanguage models. OpenAI Blog, 2024. 1\n[2] \nYunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin,\nCihang Xie, and Yuyin Zhou. A preliminary study of o1 in medicine: Are we closer to an ai\ndoctor? arXiv preprint arXiv:2409.15277, 2024. 9\n[3] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao\nWu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al. Evaluation of openai o1: Opportunities and\nchallenges of agi. arXiv preprint arXiv:2409.18486, 2024. 1\n[4] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe\nYuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1.\narXiv preprint arXiv:2410.18982, 2024. 1, 9\n[5] Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng\nGuo, Xuanjing Huang, and Xipeng Qiu. Scaling of search and learning: A roadmap to reproduce\no1 from reinforcement learning perspective. arXiv preprint arXiv:2412.14135, 2024. 9\n[6] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song,\nLei Chen, Lionel M Ni, et al. Openr: An open source framework for advanced reasoning with\nlarge language models. arXiv preprint arXiv:2410.09671, 2024. 1, 9\n[7] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. 1, 9\n[8] \nTrung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft:\nReasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. 3, 6, 8\n[9] \nDi Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei\nZhang, Marco Pavone, Yuqiang Li, et al. Llama-berry: Pairwise optimization for o1-like\nolympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024. 1, 9\n[10] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan\nZhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in\nmedicine. arXiv preprint arXiv:2404.18416, 2024. 1, 9\n[11] Vimla L Patel, José F Arocha, and Jiajie Zhang. Thinking and reasoning in medicine. The\nCambridge handbook of thinking and reasoning, 14:727–750, 2005.\n[12] Junying Chen, Chi Gui, Anningzhe Gao, Ke Ji, Xidong Wang, Xiang Wan, and Benyou\nWang. Cod, towards an interpretable medical agent using chain of diagnosis. arXiv preprint\narXiv:2407.13301, 2024. 1, 9\n[13] Shaochen Xu, Yifan Zhou, Zhengliang Liu, Zihao Wu, Tianyang Zhong, Huaqin Zhao, Yiwei\nLi, Hanqi Jiang, Yi Pan, Junhao Chen, et al. Towards next-generation medical agent: How o1 is\nreshaping decision-making in medical scenarios. arXiv preprint arXiv:2411.14461, 2024. 1\n[14] Mohamad-Hani Temsah, Amr Jamal, Khalid Alhasan, Abdulkarim A Temsah, and Khalid H\nMalki. Openai o1-preview vs. chatgpt in healthcare: A new frontier in medical ai reasoning.\nCureus, 16(10):e70640, 2024. 1, 9\n11\n\n[15] Junying Chen, Xidong Wang, Ke Ji, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang,\nDingjie Song, Wenya Xie, Chuyi Kong, et al. Huatuogpt-ii, one-stage training for medical\nadaption of llms. arXiv preprint arXiv:2311.09774, 2023. 1, 6, 9, 23\n[16] \nDaixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading\ncomprehension. In The Twelfth International Conference on Learning Representations, 2023. 1\n[17] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\ndisease does this patient have? a large-scale open domain question answering dataset from\nmedical exams. Applied Sciences, 11(14):6421, 2021. 2, 6, 23\n[18] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical domain question answering. In Conference on\nhealth, inference, and learning, pages 248–260. PMLR, 2022. 2, 6\n[19] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin,\nSurya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé,\net al. Gemma 2: Improving open language models at a practical size. arXiv preprint\narXiv:2408.00118, 2024. 3, 6\n[20] \nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024. 3, 6\n[21] Qwen Team. Qwen2.5: A party of foundation models, September 2024. 3\n[22] OpenAI. Gpt-4 technical report, 2023. 3, 9\n[23] \nKanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and\nNoah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint\narXiv:2404.03683, 2024. 3, 5, 10\n[24] Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom\nWiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving\nsparse reward tasks from scratch. In International conference on machine learning, pages\n4344–4353. PMLR, 2018. 6\n[25] Alexander Trott, Stephan Zheng, Caiming Xiong, and Richard Socher. Keeping your dis-\ntance: Solving sparse reward tasks using self-balancing shaped rewards. Advances in Neural\nInformation Processing Systems, 32, 2019. 6\n[26] \nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 6\n[27] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical domain question answering. In Conference on\nHealth, Inference, and Learning, pages 248–260. PMLR, 2022. 6\n[28] \nKaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin\nLi, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in\nbiomedicine. arXiv preprint arXiv:2406.03949, 2024. 6, 9\n[29] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo,\nWeiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and\nchallenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574,\n2024. 6\n[30] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark,\nStephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question\nanswering with large language models. arXiv preprint arXiv:2305.09617, 2023. 6, 9\n[31] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint\narXiv:2407.10671, 2024. 6\n12\n\n[32] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,\nJiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv\npreprint arXiv:2403.04652, 2024. 6\n[33] \nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 6\n[34] Malaikannan Sankarasubbu Ankit Pal and Malaikannan Sankarasubbu. Openbiollms: Advancing\nopen-source large language models for healthcare and life sciences, 2024. 6, 9\n[35] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier,\nand Richard Dufour. Biomistral: A collection of open-source pretrained large language models\nfor medical domains. arXiv preprint arXiv:2402.10373, 2024. 6, 9\n[36] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. Pubmedqa: A\ndataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. 6\n[37] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a\nbenchmark. arXiv preprint arXiv:2311.12022, 2023. 6\n[38] \nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\nAdvances in Neural Information Processing Systems, 36, 2024. 8\n[39] \nArash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier\nPietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization\nfor learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. 8\n[40] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision\nlanguage models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. 9\n[41] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao\nSang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. 9\n[42] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua\nLuo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions.\narXiv preprint arXiv:2411.14405, 2024. 9\n[43] Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng\nZhang, and Eric Horvitz. From medprompt to o1: Exploration of run-time strategies for medical\nchallenge problems and beyond. arXiv preprint arXiv:2411.03590, 2024. 9\n[44] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui\nDing, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models\nwith longtermism. arXiv preprint arXiv:2401.02954, 2024. 9\n[45] \nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 9\n[46] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung,\nNathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models\nencode clinical knowledge. Nature, 620(7972):172–180, 2023. 9\n[47] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas\nKing, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models\noutcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452,\n2023. 9\n[48] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and\nYang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents. arXiv\npreprint arXiv:2405.02957, 2024. 9\n13\n\n[49] Ming Xu. Medicalgpt: Training medical gpt model. https://github.com/shibing624/\nMedicalGPT, 2023. 9\n[50] \nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu.\nHuatuo: Tuning llama model with chinese medical knowledge, 2023.\n[51] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser,\nAlexander Löser, Daniel Truhn, and Keno K Bressem. Medalpaca–an open-source collection of\nmedical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023.\n[52] \nChaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmc-\nllama: toward building open-source language models for medicine. Journal of the American\nMedical Informatics Association, page ocae045, 2024. 9\n[53] \nZhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu, Cheng Zhong, Jiajie Peng,\nXuanjing Huang, and Zhongyu Wei. Disc-medllm: Bridging general large language models and\nreal-world medical consultation, 2023. 9\n[54] Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling Yan, Yixin Liu, Zhengliang Liu,\nLifang He, Brian Davison, Xiang Li, et al. Biomedgpt: a unified and generalist biomedical\ngenerative pre-trained transformer for vision, language, and multimodal tasks. arXiv e-prints,\npages arXiv–2305, 2023.\n[55] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen,\nXidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting\nmedical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280,\n2024.\n[56] Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao,\nXiang Wan, Haizhou Li, and Benyou Wang. Apollo: Lightweight multilingual medical llms\ntowards democratizing medical ai to 6b people. arXiv preprint arXiv:2403.03640, 2024.\n[57] Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, and Benyou Wang.\nEfficiently democratizing medical llms for 50 languages via a mixture of language family\nexperts. arXiv preprint arXiv:2410.10626, 2024.\n[58] Clément Christophe, Praveen K Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie\nRajan, Ahmed Al-Mahrooqi, Avani Gupta, Muhammad Umar Salman, Gurpreet Gosal, et al.\nMed42–evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient\napproaches. arXiv preprint arXiv:2404.14779, 2024. 9\n[59] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba,\nFrancesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami,\net al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint\narXiv:2311.16079, 2023. 9\n[60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural\nInformation Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\nDecember 9, 2022, 2022. 9\n[61] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023,\nKigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 9\n[62] \nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning\nwork? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022, pages 11048–11064. Association for\nComputational Linguistics, 2022. 9\n14\n\n[63] Yisheng Song, Ting Wang, Puyu Cai, Subrota K. Mondal, and Jyoti Prakash Sahoo. A com-\nprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities.\nACM Comput. Surv., 55(13s):271:1–271:40, 2023. 9\n[64] \nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning\nwith reasoning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and\nA. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on\nNeural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November\n28 - December 9, 2022, 2022. 9\n[65] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei\nHan. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika\nBali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1051–1068. Association for\nComputational Linguistics, 2023. 9\n[66] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross J.\nAnderson. The curse of recursion: Training on generated data makes models forget. CoRR,\nabs/2305.17493, 2023. 9\n[67] \nSina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein\nBabaei, Daniel LeJeune, Ali Siahkoohi, and Richard G. Baraniuk. Self-consuming generative\nmodels go MAD. In The Twelfth International Conference on Learning Representations, ICLR\n2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 9\n[68] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee,\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The\nTwelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria,\nMay 7-11, 2024. OpenReview.net, 2024. 10\n[69] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng,\nQingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical\nreasoning for large language models via reinforced evol-instruct. CoRR, abs/2308.09583, 2023.\n10\n[70] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:\nIterative refinement with self-feedback. In Alice Oh, Tristan Naumann, Amir Globerson, Kate\nSaenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing\nSystems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS\n2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 10\n[71] Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan,\nSamuel R. Bowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training\nwith natural language feedback. CoRR, abs/2303.16749, 2023.\n[72] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi,\nand Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh Interna-\ntional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023. 10\n[73] Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Jia Liu, Tao Gui, Qi Zhang,\nand Xuanjing Huang. Self-polish: Enhance reasoning in large language models via problem\nrefinement. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association\nfor Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 11383–\n11406. Association for Computational Linguistics, 2023.\n[74] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West,\nand Boi Faltings. REFINER: reasoning feedback on intermediate representations. In Yvette\nGraham and Matthew Purver, editors, Proceedings of the 18th Conference of the European\nChapter of the Association for Computational Linguistics, EACL 2024 - Volume 1: Long Papers,\nSt. Julian’s, Malta, March 17-22, 2024, pages 1100–1126. Association for Computational\nLinguistics, 2024. 10\n15\n\n[75] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai:\nHarmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. 10\n[76] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying\nSong, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The\nTwelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria,\nMay 7-11, 2024. OpenReview.net, 2024. 10\n[77] Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Wang. Pride\nand prejudice: LLM amplifies self-bias in self-refinement. In Lun-Wei Ku, Andre Martins,\nand Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August\n11-16, 2024, pages 15474–15492. Association for Computational Linguistics, 2024. 10\n[78] Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien, Ramakanth Pasunuru, Jane Dwivedi-\nYu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shep-\nherd: A critic for language model generation. CoRR, abs/2308.04592, 2023. 10\n[79] Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Junyang Lin,\nChang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, and Baobao Chang. LLM critics help catch\nbugs in mathematics: Towards a better mathematical verifier with natural language feedback.\nCoRR, abs/2406.14024, 2024.\n[80] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya\nJia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems\nusing GPT-4 code interpreter with code-based self-verification. In The Twelfth International\nConference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\nReview.net, 2024.\n[81] Alexander Havrilla, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu,\nMaksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. Glore: When, where, and how to\nimprove LLM reasoning via global and local refinements. In Forty-first International Conference\non Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\n10\n[82] Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han,\nDebing Zhang, and Le Sun. Critic-cot: Boosting the reasoning abilities of large language model\nvia chain-of-thoughts critic, 2024. 10\n[83] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate\nBaumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha\nShrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal M. P. Behbahani, and\nAleksandra Faust. Training language models to self-correct via reinforcement learning. CoRR,\nabs/2409.12917, 2024. 10\n[84] \nXidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying\nXiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et al. Cmb: A comprehensive\nmedical benchmark in chinese. arXiv preprint arXiv:2308.08833, 2023. 23\n[85] Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang,\nChenyu You, Zhenhua Guo, Lei Zhu, et al. Benchmarking large language models on cmexam-a\ncomprehensive chinese medical exam dataset. Advances in Neural Information Processing\nSystems, 36, 2024. 23\n[86] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and\nTimothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese.\narXiv preprint arXiv:2306.09212, 2023. 23\n16\n\nA Ethical Statement\nAlthough the proposed model is a medical LLM with complex reasoning capabilities, it may still\nproduce content that includes hallucinations or inaccuracies. Therefore, the current model is not\nsuitable for real-world applications. Consequently, we will impose strict limitations on the use of our\nmodel. The models are not permitted for use in clinical or other industry applications where such\ninaccuracies could lead to unintended consequences. We emphasize the ethical responsibility of users\nto adhere to these restrictions in order to safeguard the safety and integrity of their applications.\nB Constructing Medical Verifiable Problems\nTo construct Medical Verifiable Problems, we begin by employing small models and rule-based\nmethods to identify challenging questions. Subsequently, we leverage GPT-4o to perform data\nfiltering, isolating questions that have been suitably transformed. The prompt used for this data\nfiltering process is illustrated in Figure 5. After selecting appropriate data, we reformat multiple-\nchoice medical exam questions into open-ended verifiable problems using the prompt provided in\nFigure 6.\nThe prompt for filtering Multiple-choice Questions\n<Multiple-choice Question>\n{Question}\n{Options}\nCorrect Answer: {Answer}\n</Multiple-choice Question>\nYou are an expert in filtering and evaluating multiple-choice questions for advanced reasoning tasks. Your\njob is to evaluate a given question and determine whether it meets the following criteria:\n1. **Depth of Reasoning:** The question should require deeper reasoning. If the question appears too\nsimple, mark it as \"Too Simple.\"\n2. **Unambiguous Correct Answer:** The question must have a unique and unambiguous correct answer. If\nthe question asks for \"incorrect options\" or allows for multiple correct answers, mark it as \"Ambiguous\nAnswer.\"\n3. **Open-Ended Reformulation Feasibility:** The question should be suitable for reformatting into an\nopen-ended format. If the question cannot be easily reformulated into an open-ended problem and a clear\nground-truth answer, mark it as \"Not Reformulatable.\"\nFor each question, provide one of the following evaluations:\n- \"Pass\" (The question meets all the criteria.)\n- \"Too Simple\"\n- \"Ambiguous Answer\"\n- \"Not Reformulatable\"\nFigure 5: The prompt for filtering Multiple-choice Questions. Here, {Question} and {Options}\nrepresents the multiple-choice question and options, and {Answer} represents the correct option for\nthe multiple-choice question.\nThe prompt for reformatting multiple-choice questions to open-ended verifiable problems\nI will provide you with a multiple-choice question, and your task is to rewrite it into an open-ended question,\nalong with a standard answer. The requirements are:\n1. The question must be specific, targeting the point being tested in the original multiple-choice question.\nEnsure it is open-ended, meaning no options are provided, but there must be a definitive standard answer.\n2. Based on the correct answer from the original question, provide a concise standard answer. The answer\nshould allow for precise matching to determine whether the model’s response is correct.\nHere is the multiple-choice question for you to rewrite:\n<Multiple-choice Question>\n{Question}\n{Options}\n17\n\nCorrect Answer: {Answer}\n</Multiple-choice Question>\nPlease output the result in the following JSON format:\n“‘json\n{{\n\"Open-ended Verifiable Question\": \"...\",\n\"Standard Answer\": \"...\"\n}}\n“‘\nFigure 6: The prompt for reformatting multiple-choice questions to open-ended verifiable problems.\nHere, {Question} and {Options} represents the multiple-choice question and options, and {Answer}\nrepresents the correct option for the multiple-choice question.\nC The Prompt of Verifier\nGPT-4o serves as the verifier to assess the correctness of model-generated outputs. Using the prompt\ndepicted in Figure 7, we present GPT-4o with both the model’s output and the ground-truth answer to\nevaluate the correctness of the response. The verifier returns a Boolean value: True if the response is\naccurate and False otherwise.\nThe Prompt for Verifier\n<Model Response>\n{Model Response}\n</Model Response>\n<Reference Answer>\n{Ground-true Answer}\n</Reference Answer>\nYou are provided with a model-generated response (<Model Response>) and a reference answer (<Reference\nAnswer>). Compare the model response with the reference answer and determine its correctness. Your task\nis to simply output \"True\" if the response is correct, and \"False\" otherwise.\nFigure 7: The prompt for the GPT-4o verifier. {Model Response} represents the output of the model\nto be verified. {Ground-true Answer} represents the ground-truth answer for medical verifiable\nproblems.\nD Prompts for Searching Trajectories\nThis section outlines the prompts used for constructing complex Chain-of-Thought (CoT) reasoning\npathways. Initially, a question x is presented to GPT-4o, which generates an initial CoT response\nusing the prompt shown in Figure 8. If the verifier determines the response to be incorrect, GPT-4o\nemploys one of several search strategies to iteratively refine the output until it is accurate. The\nprompts for these four search strategies — Backtracking, Exploring New Paths, Correction, and\nVerification — are detailed in Figures 10, 10, 11, and 12, respectively.\nThe prompt for initial CoT\n<question>\n{Question}\n</question>\nPlease respond to the above question <question> using the Chain of Thought (CoT) reasoning method.\nYour response should consist of multiple steps, each of which includes three types of actions: **\"Inner\nThinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**:\n18\n\n- **’Inner Thinking’**: This is the step where thinking is done. Note that multiple ’Inner Thinking’ steps are\nrequired to describe thorough reasoning. Each step should first generate a brief title.\n- **’Final Conclusion’**: At this stage, you summarize the correct reasoning from previous ’Inner Thinking’\nsteps and provide the final answer. No title is required here.\n- **’Verification’**: At this stage, you verify the conclusion from the \"Final Conclusion\" step. If the\nconclusion holds, end the process. If not, return to \"Inner Thinking\" for further reasoning. No title is required\nhere.\nThe output format must strictly follow the JSON structure below:\n“‘json\n{\n\"CoT\": [\n{\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"},\n...,\n{\"action\": \"Final Conclusion\", \"content\": \"...\"},\n{\"action\": \"Verification\", \"content\": \"...\"}\n]\n}\n“‘\nFigure 8: The prompt for initial CoT. {Question} represents the input question, i.e., the question x\nof the medical verifiable problems.\nThe Prompt for Backtracking Breask Search Strategy\n<question>\n{Question}\n</question>\n<previous reasoning>\n{Previous_CoT}\n<previous reasoning>\n<response requirements>\nYour response must include the following steps, each composed of three types of actions: **\"Inner\nThinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**:\n1. **Inner Thinking**: Break down the reasoning process into multiple concise steps. Each step should start\nwith a brief title to clarify its purpose.\n2. **Final Conclusion**: Summarize the correct reasoning from all previous ’Inner Thinking’ steps and\nprovide the final answer. No title is needed for this section.\n3. **Verification**: Verify the accuracy of the \"Final Conclusion\". If it holds, conclude the process.\nOtherwise, return to \"Inner Thinking\" for further refinement.\n</response requirements>\n<question> represents the question to be answered, and <previous reasoning> contains your prior reasoning.\nYour task is to continue from the current ’Verification’ step. I have manually reviewed the reasoning and\ndetermined that the **Final Conclusion** is false. Your ’Verification’ results must align with mine. Proceed\nto refine the reasoning using **backtracking** to revisit earlier points of reasoning and construct a new Final\nConclusion.\n### Output Format\nStrictly follow the JSON structure below. You do not need to repeat your previous reasoning. Begin directly\nfrom the next ’Verification’ stage.\n“‘json\n{\n\"CoT\": [\n{\"action\": \"Verification\", \"content\": \"...\"},\n{\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"},\n...,\n{\"action\": \"Final Conclusion\", \"content\": \"...\"},\n{\"action\": \"Verification\", \"content\": \"...\"}\n19\n\n]\n}\n“‘\nFigure 9: The prompt for Backtracking search strategy. Here, {Question} represents the problem x\nof the medical verifiable problems, and {Previous_CoT} represents the previous chain of thought\nprocess, i.e., [e\n0\n, y\n0\n, . . . , e\ni−1\n, y\ni−1\n].\nThe Prompt for Exploring New Paths Breask Search Strategy\n<question>\n{Question}\n</question>\n<previous reasoning>\n{Previous_CoT}\n<previous reasoning>\n<response requirements>\nYour response must include the following steps, each composed of three types of actions: **\"Inner\nThinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**:\n1. **Inner Thinking**: Break down the reasoning process into multiple concise steps. Each step should start\nwith a brief title to clarify its purpose.\n2. **Final Conclusion**: Summarize the correct reasoning from all previous ’Inner Thinking’ steps and\nprovide the final answer. No title is needed for this section.\n3. **Verification**: Verify the accuracy of the \"Final Conclusion\". If it holds, conclude the process.\nOtherwise, return to \"Inner Thinking\" for further refinement.\n</response requirements>\n<question> represents the question to be answered, and <previous reasoning> contains your prior reasoning.\nYour task is to continue from the current ’Verification’ step. I have manually reviewed the reasoning and\ndetermined that the **Final Conclusion** is false. Your ’Verification’ results must align with mine. Proceed\nto refine the reasoning by exploring new approaches to solving this problem and construct a new Final\nConclusion.\n### Output Format\nStrictly follow the JSON structure below. You do not need to repeat your previous reasoning. Begin directly\nfrom the next ’Verification’ stage.\n“‘json\n{\n\"CoT\": [\n{\"action\": \"Verification\", \"content\": \"...\"},\n{\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"},\n...,\n{\"action\": \"Final Conclusion\", \"content\": \"...\"},\n{\"action\": \"Verification\", \"content\": \"...\"}\n]\n}\n“‘\nFigure 10: The prompt for Exploring New Paths search strategy. Here, {Question} represents the\nproblem x of the medical verifiable problems, and {Previous_CoT} represents the previous chain of\nthought process, i.e., [e\n0\n, y\n0\n, . . . , e\ni−1\n, y\ni−1\n].\nThe Prompt for Correction Breask Search Strategy\n<question>\n{Question}\n20\n\n</question>\n<previous reasoning>\n{Previous_CoT}\n<previous reasoning>\n<response requirements>\nYour response must include the following steps, each composed of three types of actions: **\"Inner\nThinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**:\n1. **Inner Thinking**: Break down the reasoning process into multiple concise steps. Each step should start\nwith a brief title to clarify its purpose.\n2. **Final Conclusion**: Summarize the correct reasoning from all previous ’Inner Thinking’ steps and\nprovide the final answer. No title is needed for this section.\n3. **Verification**: Verify the accuracy of the \"Final Conclusion\". If it holds, conclude the process.\nOtherwise, return to \"Inner Thinking\" for further refinement.\n</response requirements>\n<question> represents the question to be answered, and <previous reasoning> contains your prior reasoning.\nYour task is to continue from the current ’Verification’ step. I have manually reviewed the reasoning and\ndetermined that the **Final Conclusion** is false. Your ’Verification’ results must align with mine. Proceed\nto refine the reasoning by making precise **corrections** to address prior flaws and construct a new Final\nConclusion.\n### Output Format\nStrictly follow the JSON structure below. You do not need to repeat your previous reasoning. Begin directly\nfrom the next ’Verification’ stage.\n“‘json\n{\n\"CoT\": [\n{\"action\": \"Verification\", \"content\": \"...\"},\n{\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"},\n...,\n{\"action\": \"Final Conclusion\", \"content\": \"...\"},\n{\"action\": \"Verification\", \"content\": \"...\"}\n]\n}\n“‘\nFigure 11: The prompt for Correction search strategy. Here, {Question} represents the problem x\nof the medical verifiable problems, and {Previous_CoT} represents the previous chain of thought\nprocess, i.e., [e\n0\n, y\n0\n, . . . , e\ni−1\n, y\ni−1\n].\nThe Prompt for Verification Breask Search Strategy\n<question>\n{Question}\n</question>\n<previous reasoning>\n{Previous_CoT}\n<previous reasoning>\n<response requirements>\nYour response must include the following steps, each composed of three types of actions: **\"Inner\nThinking\"**, **\"Final Conclusion\"**, and **\"Verification\"**:\n1. **Inner Thinking**: Break down the reasoning process into multiple concise steps. Each step should start\nwith a brief title to clarify its purpose.\n2. **Final Conclusion**: Summarize the correct reasoning from all previous ’Inner Thinking’ steps and\nprovide the final answer. No title is needed for this section.\n21\n\n3. **Verification**: Verify the accuracy of the \"Final Conclusion\". If it holds, conclude the process.\nOtherwise, return to \"Inner Thinking\" for further refinement.\n</response requirements>\n<question> represents the question to be answered, and <previous reasoning> contains your prior reasoning.\nYour task is to continue from the current ’Verification’ step. I have manually reviewed the reasoning and\ndetermined that the **Final Conclusion** is false. Your ’Verification’ results must align with mine. Proceed\nto refine the reasoning by conducting a thorough **validation** process to ensure validity and construct a\nnew Final Conclusion.\n### Output Format\nStrictly follow the JSON structure below. You do not need to repeat your previous reasoning. Begin directly\nfrom the next ’Verification’ stage.\n“‘json\n{\n\"CoT\": [\n{\"action\": \"Verification\", \"content\": \"...\"},\n{\"action\": \"Inner Thinking\", \"title\": \"...\", \"content\": \"...\"},\n...,\n{\"action\": \"Final Conclusion\", \"content\": \"...\"},\n{\"action\": \"Verification\", \"content\": \"...\"}\n]\n}\n“‘\nFigure 12: The prompt for Verification search strategy. Here, {Question} represents the problem x\nof the medical verifiable problems, and {Previous_CoT} represents the previous chain of thought\nprocess, i.e., [e\n0\n, y\n0\n, . . . , e\ni−1\n, y\ni−1\n].\nE Prompts for Constructing SFT Training Data\nWhen a successful trajectory [e\n0\n, y\n0\n, . . . , e\ni\n, y\ni\n] is found, it is reformatted into a coherent, natural\nlanguage reasoning process ˆe (Complex CoT) using the prompt shown in Figure 13. This reformatting\navoids rigid structures, using smooth transitions (e.g., “hmm,” “also,” “wait”) to streamline reasoning\nand reduce token usage. The model then generates a formal response ˆy for for question x using the\nconclusion of ˆe with the prompt in Figure 13.\nThe prompt for reformatting a reasoning trajectory to complex CoT\n<Thought Process>\n{Thought_Process}\n</Thought Process>\n<Question>\n{Question}\n</Question>\nThe <Thought Process> above reflects the model’s reasoning based on the <Question>. Your task is to\nrewrite the <Thought Process> to resemble a more human-like, intuitive natural thinking process. The new\nversion should:\n1. Be presented as step-by-step reasoning, with each thought on a new line separated by a line break.\n2. Avoid structured titles or formatting, focusing on natural transitions. Use casual and natural language for\ntransitions or validations, such as \"hmm,\" \"oh,\" \"also,\" or \"wait.\"\n3. Expand the content, making the reasoning richer, more detailed, and logically clear while still being\nconversational and intuitive.\nReturn directly the revised natural thinking in JSON format as follows:\n“‘json\n{\n22\n\n\"NaturalReasoning\": \"...\"\n}\nFigure 13: The prompt for reformatting a reasoning trajectory to complex CoT ˆe. Here,\n{Thought_Process} represents the successful reasoning trajectory of [e\n0\n, y\n0\n, . . . , e\ni\n, y\ni\n], and\n{Question} represents the question x.\nThe prompt for generating a formal response with complex CoT\n<Internal Thinking>\n{Complex_CoT}\n</Internal Thinking>\n<Question>\n{Question}\n</Question>\nThe <Internal Thinking> represents your internal thoughts about the <Question>. Based on this, generate\na rich and high-quality final response to the user. If there is a clear answer, provide it first. Ensure your\nfinal response closely follows the <Question>. The response style should resemble GPT-4’s style as much as\npossible. Output only your final response, without any additional content.\nFigure 14: The prompt for generating a formal response ˆy with complex CoT ˆe. Here, {Complex_CoT}\nrepresents the complex CoT ˆe, and {Question} represents the question x.\nF Settings of other RL training\nwe further compared different RL-related algorithms with PPO. Specifically, we employed the\npreference-learning algorithm DPO and the REINFORCE-style algorithm RLOO.\nDPO For DPO, we had the model generate five answers for each question offline and used a verifier\nto identify pairs of one correct and one incorrect answer. If no such pairs were found, the data was\ndiscarded. Verified correct answers were used as positive examples, while failed verifications served\nas negative examples for training DPO. The hyperparameters for DPO training were set as follows:\nlearning rate of 1e-6, batch size of 128, and a regularization parameter of 1.\nRLOO For RLOO, we used the same reward function as PPO. The parameters were also identical\nto those of PPO, with an additional parameter rloo_k set to 2.\nG Chinese Medical Model\nModel Training For the Chinese medical domain, we replaced the exam questions from the CMB\ntraining set for Chinese medical verifiable problems. Based on the same training process as the English\nversion of HuatuoGPT-o1, we developed HuatuoGPT-o1-7B-zh, built on the Qwen2.5-7B-Instruct\nmodel.\nChinese Medical Evaluation To assess the Chinese medical capabilities, we evaluated the model\non three Chinese medical benchmarks, including the Chinese test set from MedQA (MCMLE) [17 ],\nthe test set from CMB-Exam [ 84 ], and the test set from CMExam [ 85]. Additionally, we evaluated the\nmodel on the medical section of the Chinese general evaluation benchmark CMMLU [86], covering\ntracks of ’clinical knowledge,’ ’agronomy,’ ’college medicine,’ ’genetics,’ ’nutrition,’ ’Traditional\nChinese Medicine,’ and ’virology’.\nComparison Models We compared the performance of three general Chinese models: Qwen2.5,\nGLM-4, and Yi. Additionally, we included a comparison with a Chinese medical model, HuatuoGPT-\n2-7B [15].\n23"
        }
      }
    ]
  },
  "settings": {
    "executionOrder": "v1",
    "saveManualExecutions": false,
    "callerPolicy": "workflowsFromSameOwner"
  },
  "staticData": null,
  "tags": [
    {
      "createdAt": "2025-01-07T21:03:39.121Z",
      "updatedAt": "2025-01-07T21:03:39.121Z",
      "id": "f2wvvOUDWEemDKxT",
      "name": "Backup"
    }
  ],
  "triggerCount": 0,
  "updatedAt": "2025-01-07T21:03:41.613Z",
  "versionId": "f229e062-a6c0-44d3-aeb6-617021e9389b"
}